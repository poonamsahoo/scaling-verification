verifier_cfg:
  verifier_type: "specific_subset"          # "reward_models", "judges", or "all"
  verifier_size: "all"          # "all", "small", "medium", "large", or integer threshold
  verifier_subset: ['QwenPRM_min_scores', 'GPM_scores', 'LDLRewardGemma_scores', 'EurusPRMStage2_avg_scores', 'QwenPRM_max_scores', 'EurusPRMStage2_max_scores', 'Qwen72B_scores', 'InternLM2Reward7B_scores', 'DeepSeekQwen32B_verdicts', 'EurusPRMStage1_avg_scores']

# Weaver Weak Supervision Configuration
# For detailed parameter descriptions, see selection/README.md

data_cfg:
  dataset_name: null # Options: "MATH-500", "MMLU-Pro", "MMLU", "GPQA", "GPQA-Diamond", "GPQA-1K" (ignored if dataset_path is set)
  dataset_path: "amyguan/math500-k50-dev" # "amyguan/math500-k50-test" # Optional: custom dataset path (HF hub or local disk)
  train_split: 1.0 # 1.0 = no test set, <1.0 = fraction for training
  train_queries: 1 # Fraction or number of training queries to use
  train_samples: 1 # Fraction of samples per problem to use
  same_train_test: false
  nan_replacement: 0 # Replace NaN with: 0 or "mean"
  random_seed: 0
  model_size: null # For dataset selection: "8B" or "70B" (not required if dataset_path defined)
  reward_threshold: 0.5 # Binarization threshold, null = no threshold
  save_weaver_scores: true # Save Weaver scores as column in custom dataset
  
  # Score normalization
  normalize_type: "all_problems" # Options: "per_problem", "all_problems"
  normalize_method: "minmax" # Options: "minmax", "quantile", "winsorize"
  normalize_params: ${normalize_method_params.${data_cfg.normalize_method}}
  
  # Problem similarity (for per-problem models)
  closest_train_problem_method: "mean_verifier_distance" # Options: "SBERT", "mean_verifier_distance"
  closest_train_problem_metric_type: "euclidean" # Options: "cosine", "euclidean"
  
  verifier_cfg: ${verifier_cfg}
  mv_as_verifier: false # Include majority vote as verifier
  fixed_test_split: null

debug: false
max_samples_per_problem: null # Optional: limit samples per problem

model_cfg:
  model_type: "weak_supervision"
  model_class: "per_dataset" # Options: "per_problem", "per_dataset"
  model_params: ${model_params.${model_cfg.model_type}}

fit_cfg:
  fit_type: "wclosest_to_train" # Options: "wclosest_to_train", "search_weights"

# now_str: ${now:%Y%m%d_%H%M%S}
log_dataset_name: ${data_cfg.dataset_path}
log_dataset_dev: "val"
logging: "wandb"
k: null
alpha: null
beta: null
gamma: null
wandb_cfg:
  project: "verification"
  entity: 329a # "${oc.env:WANDB_ENTITY}"
  name: "${log_dataset_name}-${log_dataset_dev}-subset" # -${now_str} auto increment name

# Model-specific parameters
model_params:
  weak_supervision:
    k: 2 # Number of classes
    seed: 0
    binarize_threshold: 0.5
    metric: "scores"
    n_epochs: 1000
    mu_epochs: 1000
    log_train_every: 1000
    lr: 0.00001
    use_deps: "drop" # Options: "none", "drop", "model"
    use_label_on_test: true
    drop_imbalanced_verifiers: "all" # Options: null, "all", "small", "large"
    drop_k: 100 # Number of verifiers to select
    cb_args:
      class_balance: "labels" # Options: "labels" or float value

  naive_ensemble:
    tmp: # No parameters required

  first_sample:
    tmp: # No parameters required

# Normalization method parameters
normalize_method_params:
  minmax:
    tmp: # No parameters required
    
  quantile:
    output_distribution: "uniform"
    n_quantiles: 100
    
  winsorize:
    lower_quantile: 0.05
    upper_quantile: 0.95